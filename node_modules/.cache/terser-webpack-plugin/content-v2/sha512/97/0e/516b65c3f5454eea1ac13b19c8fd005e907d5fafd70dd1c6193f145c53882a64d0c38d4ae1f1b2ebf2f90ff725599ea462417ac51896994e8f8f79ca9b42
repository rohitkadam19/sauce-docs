{"code":"(window.webpackJsonp=window.webpackJsonp||[]).push([[4],{205:function(e,t,n){\"use strict\";n.r(t),n.d(t,\"MDXContext\",(function(){return d})),n.d(t,\"MDXProvider\",(function(){return h})),n.d(t,\"mdx\",(function(){return f})),n.d(t,\"useMDXComponents\",(function(){return m})),n.d(t,\"withMDXComponents\",(function(){return u}));var r=n(0),i=n.n(r);function s(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(){return(o=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var r in n)Object.prototype.hasOwnProperty.call(n,r)&&(e[r]=n[r])}return e}).apply(this,arguments)}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){s(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,r,i=function(e,t){if(null==e)return{};var n,r,i={},s=Object.keys(e);for(r=0;r<s.length;r++)n=s[r],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(r=0;r<s.length;r++)n=s[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var d=i.a.createContext({}),u=function(e){return function(t){var n=m(t.components);return i.a.createElement(e,o({},t,{components:n}))}},m=function(e){var t=i.a.useContext(d),n=t;return e&&(n=\"function\"==typeof e?e(t):l(l({},t),e)),n},h=function(e){var t=m(e.components);return i.a.createElement(d.Provider,{value:t},e.children)},p={inlineCode:\"code\",wrapper:function(e){var t=e.children;return i.a.createElement(i.a.Fragment,{},t)}},b=i.a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,s=e.originalType,o=e.parentName,a=c(e,[\"components\",\"mdxType\",\"originalType\",\"parentName\"]),d=m(n),u=r,h=d[\"\".concat(o,\".\").concat(u)]||d[u]||p[u]||s;return n?i.a.createElement(h,l(l({ref:t},a),{},{components:n})):i.a.createElement(h,l({ref:t},a))}));function f(e,t){var n=arguments,r=t&&t.mdxType;if(\"string\"==typeof e||r){var s=n.length,o=new Array(s);o[0]=b;var a={};for(var l in t)hasOwnProperty.call(t,l)&&(a[l]=t[l]);a.originalType=e,a.mdxType=\"string\"==typeof e?e:r,o[1]=a;for(var c=2;c<s;c++)o[c]=n[c];return i.a.createElement.apply(null,o)}return i.a.createElement.apply(null,n)}b.displayName=\"MDXCreateElement\"},36:function(e,t,n){\"use strict\";n.r(t),n.d(t,\"frontMatter\",(function(){return o})),n.d(t,\"metadata\",(function(){return a})),n.d(t,\"rightToc\",(function(){return l})),n.d(t,\"default\",(function(){return d}));var r=n(3),i=n(8),s=(n(0),n(205)),o={id:\"trends\",title:\"Comparing Statistical Trends\",sidebar_label:\"Trends\",description:\"See how grouping tests reveals outcome patterns across isolated variables, such as browser, operating system, or date to optimize your tests.\"},a={unversionedId:\"insights/trends\",id:\"insights/trends\",isDocsHomePage:!1,title:\"Comparing Statistical Trends\",description:\"See how grouping tests reveals outcome patterns across isolated variables, such as browser, operating system, or date to optimize your tests.\",source:\"@site/docs/insights/trends.md\",slug:\"/insights/trends\",permalink:\"/insights/trends\",editUrl:\"https://github.com/saucelabs/sauce-docs/edit/master/docs/insights/trends.md\",version:\"current\",lastUpdatedBy:\"sweeneyskirt-sl\",lastUpdatedAt:1608152084,sidebar_label:\"Trends\",sidebar:\"someSidebar\",previous:{title:\"Evaluating a Test Over Time\",permalink:\"/insights/history\"},next:{title:\"Analyzing Failure Patterns Across Your Test Suite\",permalink:\"/insights/failure-analysis\"}},l=[{value:\"Drilling Down on Visualizations\",id:\"drilling-down-on-visualizations\",children:[]},{value:\"Using Trends Data to Improve Testing\",id:\"using-trends-data-to-improve-testing\",children:[{value:\"Comparing Test Results on Chrome 50 and 55\",id:\"comparing-test-results-on-chrome-50-and-55\",children:[]},{value:\"Using the Efficiency Metric to Optimize Tests\",id:\"using-the-efficiency-metric-to-optimize-tests\",children:[]}]}],c={rightToc:l};function d(e){var t=e.components,n=Object(i.a)(e,[\"components\"]);return Object(s.mdx)(\"wrapper\",Object(r.default)({},c,n,{components:t,mdxType:\"MDXLayout\"}),Object(s.mdx)(\"p\",null,\"The Trends section of the Insights feature provides a variety of data visualizations to give you a holistic perspective of your test outcomes. The following table describes each section.\"),Object(s.mdx)(\"table\",null,Object(s.mdx)(\"tr\",null,Object(s.mdx)(\"th\",null,\"Section\"),Object(s.mdx)(\"th\",null,\"Statistical Information\")),Object(s.mdx)(\"tr\",null,Object(s.mdx)(\"td\",null,Object(s.mdx)(\"b\",null,\"Number of Tests\")),Object(s.mdx)(\"td\",null,\"The total number of tests run during the specified time period, separated in increments relative to the overall duration. For example, increments may be every 10 minutes for a time period of one hour, while increments might be daily for a 30 day time period.\")),Object(s.mdx)(\"tr\",null,Object(s.mdx)(\"td\",null,Object(s.mdx)(\"b\",null,\"Pass/Fail Rate\")),Object(s.mdx)(\"td\",null,\"For each increment in the time period, the percentage of tests that:\",Object(s.mdx)(\"br\",null),Object(s.mdx)(\"ul\",null,Object(s.mdx)(\"li\",null,Object(s.mdx)(\"b\",null,\"Completed\"),\": Ran to completion, but did not have a pass or fail status.\"),Object(s.mdx)(\"li\",null,Object(s.mdx)(\"b\",null,\"Passed\"),\": Ran to completion with a status of Passed.\"),Object(s.mdx)(\"li\",null,Object(s.mdx)(\"b\",null,\"Failed\"),\": Ran to completion with a status of Failed.\"),Object(s.mdx)(\"li\",null,Object(s.mdx)(\"b\",null,\"Errored\"),\": Did not run to completion due to a fatal error.\")))),Object(s.mdx)(\"tr\",null,Object(s.mdx)(\"td\",null,Object(s.mdx)(\"b\",null,\"Number of Errors\")),Object(s.mdx)(\"td\",null,\"The total number or errors that occurred during the specified time period, sorted by individual error message.\")),Object(s.mdx)(\"tr\",null,Object(s.mdx)(\"td\",null,Object(s.mdx)(\"b\",null,\"Build and Test Statistics\")),Object(s.mdx)(\"td\",null,\"A snapshot of all tests run during the time period, displayed in separate tabs based on whether the test is or is not assigned a Build ID.For each test listed, basic data about the time the test was executed, the time it took to run, the Sauce Labs user who ran it, and its outcome. Tests in the \",Object(s.mdx)(\"b\",null,\"Builds\"),\" tab have an additional statistic - \",Object(s.mdx)(\"i\",null,\"Efficiency\"),\", that indicates whether the tests in the build run in parallel to optimize the execution time for the entire build.\",Object(s.mdx)(\"br\",null),\"This visualization can be further filtered to show only only tests with a failed and/or errored status.\"))),Object(s.mdx)(\"h2\",{id:\"drilling-down-on-visualizations\"},\"Drilling Down on Visualizations\"),Object(s.mdx)(\"p\",null,\"The visualizations shown in the Trends section of Insights are interactive; you can hover over any of the bars to view a statistics overview for that increment, or you can click-drag across the bars to redraw the graph for a narrower time period. The latter action updates the Time Period filter at the top of the page accordingly. After drilling down on a time period, click the Back link to step back through the previous time periods.\"),Object(s.mdx)(\"h2\",{id:\"using-trends-data-to-improve-testing\"},\"Using Trends Data to Improve Testing\"),Object(s.mdx)(\"p\",null,\"The trend visualizations can provide you with a quick overview of what's going on with your tests as a whole, and applying filters to the visualizations enables you to dig into the data to generate answers to specific questions about test performance. Let's look at an example showing how we can use these tools to answer real questions about our site and our tests.\"),Object(s.mdx)(\"h3\",{id:\"comparing-test-results-on-chrome-50-and-55\"},\"Comparing Test Results on Chrome 50 and 55\"),Object(s.mdx)(\"p\",null,\"To find out how well a site under test performs against a browser update, we start by filtering our data to isolate only the relevant tests -- those that are owned by the same organization; were run over the past 7 days on Windows 7 for Chrome 55 and Chrome 50. This is a typical use case to compare a set of new tests for a recent browser release against the baseline of an established set of tests for a previous version of the same browser.\"),Object(s.mdx)(\"h4\",{id:\"chrome-50\"},\"Chrome 50\"),Object(s.mdx)(\"p\",null,\"As the figure below shows, more than 3,500 tests were run on Windows 7 for Chrome 50 in the past 7 days, with a 41% pass rate.\"),Object(s.mdx)(\"img\",{src:\"/static/img/insights/chrome50.png\",alt:\"Chrome 50 Trends\",width:\"750\"}),Object(s.mdx)(\"p\",null,\"There are no errors, indicating that this is a robust set of tests, but a large number of tests ran to completion without reporting a Pass or Fail status. Hovering over one of the bars in the graph shows that these no-status completions account for about 65% of the tests in every time increment. While the tests themselves perform well, it's difficult to judge how well the site functions when completed tests do not offer a definitive outcome. To provide a better baseline for cross-browser comparison, \",Object(s.mdx)(\"a\",Object(r.default)({parentName:\"p\"},{href:\"https://wiki.saucelabs.com/display/DOCS/Test+Configuration+and+Annotation\"}),\"annotate\"),\" these tests with relevant status using the Jobs API or the Selenium Javascript Executor.\"),Object(s.mdx)(\"h4\",{id:\"chrome-55\"},\"Chrome 55\"),Object(s.mdx)(\"p\",null,\"Now let's change the Browser filter to Chrome 55where nearly 4000 tests ran during the same 7 day period.\"),Object(s.mdx)(\"img\",{src:\"/static/img/insights/chrome55.png\",alt:\"Chrome 55 Trends\",width:\"750\"}),Object(s.mdx)(\"p\",null,\"The pass rate for these tests is lower than Chrome 55, at 32%, and we still see a lot of completed tests without status, but we also see some failures showing up, although there are no errors, so the tests themselves seem to executing successfully for both browser versions.\"),Object(s.mdx)(\"h4\",{id:\"conclusions\"},\"Conclusions\"),Object(s.mdx)(\"p\",null,\"In summary, we see from this browser version comparison:\"),Object(s.mdx)(\"ul\",null,Object(s.mdx)(\"li\",{parentName:\"ul\"},\"Overall coverage is consistent.\"),Object(s.mdx)(\"li\",{parentName:\"ul\"},\"Test performance is good for both, with low error rates.\"),Object(s.mdx)(\"li\",{parentName:\"ul\"},\"Site functionality is ambiguous for both browsers due to the high number of completed tests with no status.\")),Object(s.mdx)(\"h4\",{id:\"next-steps\"},\"Next Steps\"),Object(s.mdx)(\"p\",null,\"Since our comparison suggests that the tests themselves are strong, but yielded some uncertainty about how well the site performs in either Chrome version, we isolate the time interval that contains the first failing test at 4:00PM on February 7th and can drill down to the 5 minute scale to find the exact test that failed.\"),Object(s.mdx)(\"img\",{src:\"/static/img/insights/5sec_interval.png\",alt:\"Failure Interval\",width:\"600\"}),Object(s.mdx)(\"p\",null,\"Once we have isolated the failing test, we can check the \",Object(s.mdx)(\"strong\",{parentName:\"p\"},\"Builds\"),\" list and find the failing test, \",Object(s.mdx)(\"strong\",{parentName:\"p\"},\"TestCompareBrowserVersion\"),\". Click that test name to see the \",Object(s.mdx)(\"strong\",{parentName:\"p\"},\"Test Details\"),\" page, where you can review the videos, screenshots, logs, and metadata that can help you determine why the test failed for Chrome 55.\"),Object(s.mdx)(\"img\",{src:\"/static/img/insights/test_fail.png\",alt:\"Failing Test\",width:\"600\"}),Object(s.mdx)(\"h3\",{id:\"using-the-efficiency-metric-to-optimize-tests\"},\"Using the Efficiency Metric to Optimize Tests\"),Object(s.mdx)(\"p\",null,\"The \",Object(s.mdx)(\"strong\",{parentName:\"p\"},\"Builds and Test Statistics\"),\" section of the Trends page provides an \",Object(s.mdx)(\"strong\",{parentName:\"p\"},\"Efficiency\"),\" metric for builds that indicated the percentage of tests in the build that are running in parallel.\"))}d.isMDXComponent=!0}}]);","name":"0439ebed.714ae7f1.js","input":"(window[\"webpackJsonp\"] = window[\"webpackJsonp\"] || []).push([[4],{\n\n/***/ 205:\n/***/ (function(module, __webpack_exports__, __webpack_require__) {\n\n\"use strict\";\n__webpack_require__.r(__webpack_exports__);\n/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, \"MDXContext\", function() { return MDXContext; });\n/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, \"MDXProvider\", function() { return MDXProvider; });\n/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, \"mdx\", function() { return createElement; });\n/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, \"useMDXComponents\", function() { return useMDXComponents; });\n/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, \"withMDXComponents\", function() { return withMDXComponents; });\n/* harmony import */ var react__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(0);\n/* harmony import */ var react__WEBPACK_IMPORTED_MODULE_0___default = /*#__PURE__*/__webpack_require__.n(react__WEBPACK_IMPORTED_MODULE_0__);\n\n\nfunction _defineProperty(obj, key, value) {\n  if (key in obj) {\n    Object.defineProperty(obj, key, {\n      value: value,\n      enumerable: true,\n      configurable: true,\n      writable: true\n    });\n  } else {\n    obj[key] = value;\n  }\n\n  return obj;\n}\n\nfunction _extends() {\n  _extends = Object.assign || function (target) {\n    for (var i = 1; i < arguments.length; i++) {\n      var source = arguments[i];\n\n      for (var key in source) {\n        if (Object.prototype.hasOwnProperty.call(source, key)) {\n          target[key] = source[key];\n        }\n      }\n    }\n\n    return target;\n  };\n\n  return _extends.apply(this, arguments);\n}\n\nfunction ownKeys(object, enumerableOnly) {\n  var keys = Object.keys(object);\n\n  if (Object.getOwnPropertySymbols) {\n    var symbols = Object.getOwnPropertySymbols(object);\n    if (enumerableOnly) symbols = symbols.filter(function (sym) {\n      return Object.getOwnPropertyDescriptor(object, sym).enumerable;\n    });\n    keys.push.apply(keys, symbols);\n  }\n\n  return keys;\n}\n\nfunction _objectSpread2(target) {\n  for (var i = 1; i < arguments.length; i++) {\n    var source = arguments[i] != null ? arguments[i] : {};\n\n    if (i % 2) {\n      ownKeys(Object(source), true).forEach(function (key) {\n        _defineProperty(target, key, source[key]);\n      });\n    } else if (Object.getOwnPropertyDescriptors) {\n      Object.defineProperties(target, Object.getOwnPropertyDescriptors(source));\n    } else {\n      ownKeys(Object(source)).forEach(function (key) {\n        Object.defineProperty(target, key, Object.getOwnPropertyDescriptor(source, key));\n      });\n    }\n  }\n\n  return target;\n}\n\nfunction _objectWithoutPropertiesLoose(source, excluded) {\n  if (source == null) return {};\n  var target = {};\n  var sourceKeys = Object.keys(source);\n  var key, i;\n\n  for (i = 0; i < sourceKeys.length; i++) {\n    key = sourceKeys[i];\n    if (excluded.indexOf(key) >= 0) continue;\n    target[key] = source[key];\n  }\n\n  return target;\n}\n\nfunction _objectWithoutProperties(source, excluded) {\n  if (source == null) return {};\n\n  var target = _objectWithoutPropertiesLoose(source, excluded);\n\n  var key, i;\n\n  if (Object.getOwnPropertySymbols) {\n    var sourceSymbolKeys = Object.getOwnPropertySymbols(source);\n\n    for (i = 0; i < sourceSymbolKeys.length; i++) {\n      key = sourceSymbolKeys[i];\n      if (excluded.indexOf(key) >= 0) continue;\n      if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue;\n      target[key] = source[key];\n    }\n  }\n\n  return target;\n}\n\nvar isFunction = function isFunction(obj) {\n  return typeof obj === 'function';\n};\n\nvar MDXContext = /*#__PURE__*/react__WEBPACK_IMPORTED_MODULE_0___default.a.createContext({});\nvar withMDXComponents = function withMDXComponents(Component) {\n  return function (props) {\n    var allComponents = useMDXComponents(props.components);\n    return /*#__PURE__*/react__WEBPACK_IMPORTED_MODULE_0___default.a.createElement(Component, _extends({}, props, {\n      components: allComponents\n    }));\n  };\n};\nvar useMDXComponents = function useMDXComponents(components) {\n  var contextComponents = react__WEBPACK_IMPORTED_MODULE_0___default.a.useContext(MDXContext);\n  var allComponents = contextComponents;\n\n  if (components) {\n    allComponents = isFunction(components) ? components(contextComponents) : _objectSpread2(_objectSpread2({}, contextComponents), components);\n  }\n\n  return allComponents;\n};\nvar MDXProvider = function MDXProvider(props) {\n  var allComponents = useMDXComponents(props.components);\n  return /*#__PURE__*/react__WEBPACK_IMPORTED_MODULE_0___default.a.createElement(MDXContext.Provider, {\n    value: allComponents\n  }, props.children);\n};\n\nvar TYPE_PROP_NAME = 'mdxType';\nvar DEFAULTS = {\n  inlineCode: 'code',\n  wrapper: function wrapper(_ref) {\n    var children = _ref.children;\n    return /*#__PURE__*/react__WEBPACK_IMPORTED_MODULE_0___default.a.createElement(react__WEBPACK_IMPORTED_MODULE_0___default.a.Fragment, {}, children);\n  }\n};\nvar MDXCreateElement = /*#__PURE__*/react__WEBPACK_IMPORTED_MODULE_0___default.a.forwardRef(function (props, ref) {\n  var propComponents = props.components,\n      mdxType = props.mdxType,\n      originalType = props.originalType,\n      parentName = props.parentName,\n      etc = _objectWithoutProperties(props, [\"components\", \"mdxType\", \"originalType\", \"parentName\"]);\n\n  var components = useMDXComponents(propComponents);\n  var type = mdxType;\n  var Component = components[\"\".concat(parentName, \".\").concat(type)] || components[type] || DEFAULTS[type] || originalType;\n\n  if (propComponents) {\n    return /*#__PURE__*/react__WEBPACK_IMPORTED_MODULE_0___default.a.createElement(Component, _objectSpread2(_objectSpread2({\n      ref: ref\n    }, etc), {}, {\n      components: propComponents\n    }));\n  }\n\n  return /*#__PURE__*/react__WEBPACK_IMPORTED_MODULE_0___default.a.createElement(Component, _objectSpread2({\n    ref: ref\n  }, etc));\n});\nMDXCreateElement.displayName = 'MDXCreateElement';\nfunction createElement (type, props) {\n  var args = arguments;\n  var mdxType = props && props.mdxType;\n\n  if (typeof type === 'string' || mdxType) {\n    var argsLength = args.length;\n    var createElementArgArray = new Array(argsLength);\n    createElementArgArray[0] = MDXCreateElement;\n    var newProps = {};\n\n    for (var key in props) {\n      if (hasOwnProperty.call(props, key)) {\n        newProps[key] = props[key];\n      }\n    }\n\n    newProps.originalType = type;\n    newProps[TYPE_PROP_NAME] = typeof type === 'string' ? type : mdxType;\n    createElementArgArray[1] = newProps;\n\n    for (var i = 2; i < argsLength; i++) {\n      createElementArgArray[i] = args[i];\n    }\n\n    return react__WEBPACK_IMPORTED_MODULE_0___default.a.createElement.apply(null, createElementArgArray);\n  }\n\n  return react__WEBPACK_IMPORTED_MODULE_0___default.a.createElement.apply(null, args);\n}\n\n\n\n\n/***/ }),\n\n/***/ 36:\n/***/ (function(module, __webpack_exports__, __webpack_require__) {\n\n\"use strict\";\n__webpack_require__.r(__webpack_exports__);\n/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, \"frontMatter\", function() { return frontMatter; });\n/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, \"metadata\", function() { return metadata; });\n/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, \"rightToc\", function() { return rightToc; });\n/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, \"default\", function() { return MDXContent; });\n/* harmony import */ var _Users_nancysweeney_GH_sauce_docs_node_modules_babel_runtime_helpers_esm_extends__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(3);\n/* harmony import */ var _Users_nancysweeney_GH_sauce_docs_node_modules_babel_runtime_helpers_esm_objectWithoutPropertiesLoose__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(8);\n/* harmony import */ var react__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(0);\n/* harmony import */ var react__WEBPACK_IMPORTED_MODULE_2___default = /*#__PURE__*/__webpack_require__.n(react__WEBPACK_IMPORTED_MODULE_2__);\n/* harmony import */ var _mdx_js_react__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(205);\nvar frontMatter={id:'trends',title:'Comparing Statistical Trends',sidebar_label:'Trends',description:'See how grouping tests reveals outcome patterns across isolated variables, such as browser, operating system, or date to optimize your tests.'};var metadata={\"unversionedId\":\"insights/trends\",\"id\":\"insights/trends\",\"isDocsHomePage\":false,\"title\":\"Comparing Statistical Trends\",\"description\":\"See how grouping tests reveals outcome patterns across isolated variables, such as browser, operating system, or date to optimize your tests.\",\"source\":\"@site/docs/insights/trends.md\",\"slug\":\"/insights/trends\",\"permalink\":\"/insights/trends\",\"editUrl\":\"https://github.com/saucelabs/sauce-docs/edit/master/docs/insights/trends.md\",\"version\":\"current\",\"lastUpdatedBy\":\"sweeneyskirt-sl\",\"lastUpdatedAt\":1608152084,\"sidebar_label\":\"Trends\",\"sidebar\":\"someSidebar\",\"previous\":{\"title\":\"Evaluating a Test Over Time\",\"permalink\":\"/insights/history\"},\"next\":{\"title\":\"Analyzing Failure Patterns Across Your Test Suite\",\"permalink\":\"/insights/failure-analysis\"}};/* @jsxRuntime classic */ /* @jsx mdx */var rightToc=[{value:'Drilling Down on Visualizations',id:'drilling-down-on-visualizations',children:[]},{value:'Using Trends Data to Improve Testing',id:'using-trends-data-to-improve-testing',children:[{value:'Comparing Test Results on Chrome 50 and 55',id:'comparing-test-results-on-chrome-50-and-55',children:[]},{value:'Using the Efficiency Metric to Optimize Tests',id:'using-the-efficiency-metric-to-optimize-tests',children:[]}]}];var layoutProps={rightToc:rightToc};var MDXLayout=\"wrapper\";function MDXContent(_ref){var components=_ref.components,props=Object(_Users_nancysweeney_GH_sauce_docs_node_modules_babel_runtime_helpers_esm_objectWithoutPropertiesLoose__WEBPACK_IMPORTED_MODULE_1__[/* default */ \"a\"])(_ref,[\"components\"]);return Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(MDXLayout,Object(_Users_nancysweeney_GH_sauce_docs_node_modules_babel_runtime_helpers_esm_extends__WEBPACK_IMPORTED_MODULE_0__[\"default\"])({},layoutProps,props,{components:components,mdxType:\"MDXLayout\"}),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"p\",null,\"The Trends section of the Insights feature provides a variety of data visualizations to give you a holistic perspective of your test outcomes. The following table describes each section.\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"table\",null,Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"tr\",null,Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"th\",null,\"Section\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"th\",null,\"Statistical Information\")),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"tr\",null,Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"td\",null,Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"b\",null,\"Number of Tests\")),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"td\",null,\"The total number of tests run during the specified time period, separated in increments relative to the overall duration. For example, increments may be every 10 minutes for a time period of one hour, while increments might be daily for a 30 day time period.\")),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"tr\",null,Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"td\",null,Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"b\",null,\"Pass/Fail Rate\")),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"td\",null,\"For each increment in the time period, the percentage of tests that:\",Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"br\",null),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"ul\",null,Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"li\",null,Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"b\",null,\"Completed\"),\": Ran to completion, but did not have a pass or fail status.\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"li\",null,Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"b\",null,\"Passed\"),\": Ran to completion with a status of Passed.\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"li\",null,Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"b\",null,\"Failed\"),\": Ran to completion with a status of Failed.\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"li\",null,Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"b\",null,\"Errored\"),\": Did not run to completion due to a fatal error.\")))),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"tr\",null,Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"td\",null,Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"b\",null,\"Number of Errors\")),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"td\",null,\"The total number or errors that occurred during the specified time period, sorted by individual error message.\")),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"tr\",null,Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"td\",null,Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"b\",null,\"Build and Test Statistics\")),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"td\",null,\"A snapshot of all tests run during the time period, displayed in separate tabs based on whether the test is or is not assigned a Build ID.For each test listed, basic data about the time the test was executed, the time it took to run, the Sauce Labs user who ran it, and its outcome. Tests in the \",Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"b\",null,\"Builds\"),\" tab have an additional statistic - \",Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"i\",null,\"Efficiency\"),\", that indicates whether the tests in the build run in parallel to optimize the execution time for the entire build.\",Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"br\",null),\"This visualization can be further filtered to show only only tests with a failed and/or errored status.\"))),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"h2\",{\"id\":\"drilling-down-on-visualizations\"},\"Drilling Down on Visualizations\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"p\",null,\"The visualizations shown in the Trends section of Insights are interactive; you can hover over any of the bars to view a statistics overview for that increment, or you can click-drag across the bars to redraw the graph for a narrower time period. The latter action updates the Time Period filter at the top of the page accordingly. After drilling down on a time period, click the Back link to step back through the previous time periods.\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"h2\",{\"id\":\"using-trends-data-to-improve-testing\"},\"Using Trends Data to Improve Testing\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"p\",null,\"The trend visualizations can provide you with a quick overview of what's going on with your tests as a whole, and applying filters to the visualizations enables you to dig into the data to generate answers to specific questions about test performance. Let's look at an example showing how we can use these tools to answer real questions about our site and our tests.\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"h3\",{\"id\":\"comparing-test-results-on-chrome-50-and-55\"},\"Comparing Test Results on Chrome 50 and 55\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"p\",null,\"To find out how well a site under test performs against a browser update, we start by filtering our data to isolate only the relevant tests -- those that are owned by the same organization; were run over the past 7 days on Windows 7 for Chrome 55 and Chrome 50. This is a typical use case to compare a set of new tests for a recent browser release against the baseline of an established set of tests for a previous version of the same browser.\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"h4\",{\"id\":\"chrome-50\"},\"Chrome 50\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"p\",null,\"As the figure below shows, more than 3,500 tests were run on Windows 7 for Chrome 50 in the past 7 days, with a 41% pass rate.\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"img\",{src:\"/static/img/insights/chrome50.png\",alt:\"Chrome 50 Trends\",width:\"750\"}),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"p\",null,\"There are no errors, indicating that this is a robust set of tests, but a large number of tests ran to completion without reporting a Pass or Fail status. Hovering over one of the bars in the graph shows that these no-status completions account for about 65% of the tests in every time increment. While the tests themselves perform well, it's difficult to judge how well the site functions when completed tests do not offer a definitive outcome. To provide a better baseline for cross-browser comparison, \",Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"a\",Object(_Users_nancysweeney_GH_sauce_docs_node_modules_babel_runtime_helpers_esm_extends__WEBPACK_IMPORTED_MODULE_0__[\"default\"])({parentName:\"p\"},{\"href\":\"https://wiki.saucelabs.com/display/DOCS/Test+Configuration+and+Annotation\"}),\"annotate\"),\" these tests with relevant status using the Jobs API or the Selenium Javascript Executor.\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"h4\",{\"id\":\"chrome-55\"},\"Chrome 55\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"p\",null,\"Now let's change the Browser filter to Chrome 55where nearly 4000 tests ran during the same 7 day period.\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"img\",{src:\"/static/img/insights/chrome55.png\",alt:\"Chrome 55 Trends\",width:\"750\"}),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"p\",null,\"The pass rate for these tests is lower than Chrome 55, at 32%, and we still see a lot of completed tests without status, but we also see some failures showing up, although there are no errors, so the tests themselves seem to executing successfully for both browser versions.\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"h4\",{\"id\":\"conclusions\"},\"Conclusions\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"p\",null,\"In summary, we see from this browser version comparison:\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"ul\",null,Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"li\",{parentName:\"ul\"},\"Overall coverage is consistent.\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"li\",{parentName:\"ul\"},\"Test performance is good for both, with low error rates.\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"li\",{parentName:\"ul\"},\"Site functionality is ambiguous for both browsers due to the high number of completed tests with no status.\")),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"h4\",{\"id\":\"next-steps\"},\"Next Steps\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"p\",null,\"Since our comparison suggests that the tests themselves are strong, but yielded some uncertainty about how well the site performs in either Chrome version, we isolate the time interval that contains the first failing test at 4:00PM on February 7th and can drill down to the 5 minute scale to find the exact test that failed.\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"img\",{src:\"/static/img/insights/5sec_interval.png\",alt:\"Failure Interval\",width:\"600\"}),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"p\",null,\"Once we have isolated the failing test, we can check the \",Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"strong\",{parentName:\"p\"},\"Builds\"),\" list and find the failing test, \",Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"strong\",{parentName:\"p\"},\"TestCompareBrowserVersion\"),\". Click that test name to see the \",Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"strong\",{parentName:\"p\"},\"Test Details\"),\" page, where you can review the videos, screenshots, logs, and metadata that can help you determine why the test failed for Chrome 55.\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"img\",{src:\"/static/img/insights/test_fail.png\",alt:\"Failing Test\",width:\"600\"}),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"h3\",{\"id\":\"using-the-efficiency-metric-to-optimize-tests\"},\"Using the Efficiency Metric to Optimize Tests\"),Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"p\",null,\"The \",Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"strong\",{parentName:\"p\"},\"Builds and Test Statistics\"),\" section of the Trends page provides an \",Object(_mdx_js_react__WEBPACK_IMPORTED_MODULE_3__[\"mdx\"])(\"strong\",{parentName:\"p\"},\"Efficiency\"),\" metric for builds that indicated the percentage of tests in the build that are running in parallel.\"));};MDXContent.isMDXComponent=true;\n\n/***/ })\n\n}]);","inputSourceMap":null}